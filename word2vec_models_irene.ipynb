{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we are trying to train a lot of different models using word2vec as an encriptor for the X (the descriptions).\n",
    "\n",
    "The document is structred as follows:\n",
    "\n",
    "- **Load data**\n",
    "    - Descriptions (x)\n",
    "    - Features (y)\n",
    "    \n",
    "- **Word2Vec (for descriptions)**\n",
    "- **PCA (for features)**\n",
    "\n",
    "- **Model(s)** training *make sure to comment the best results of each method with its hyperparameters*\n",
    "    1. MLP\n",
    "    2. XX\n",
    "\n",
    "- **Accuracy in the validation set**\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "Ideas of things to try:\n",
    "\n",
    "- N grams\n",
    "- Keras\n",
    "- Using Tags! [done]\n",
    "- use tags and descriptions at the same time\n",
    "- Ensambled method\n",
    "\n",
    "\n",
    "- Only pairwise similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "\n",
    "def parse_tags(tags):\n",
    "    result = []\n",
    "    for doc in tags:\n",
    "        doc = doc.strip('\\n').split('\\n')\n",
    "        cat_it = []\n",
    "        if doc[0] == '':\n",
    "            result.append('no tag')\n",
    "        else:\n",
    "            for tag in doc:\n",
    "                split_tag = tag.split(':')\n",
    "                cat_it.append(split_tag[0])\n",
    "                cat_it.append(split_tag[1])\n",
    "            parsed = (' ').join(list(set(cat_it)))\n",
    "            result.append(parsed)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# function to preprocess data\n",
    "def preprocessing(data):\n",
    "    stop_words = set(stopwords.words('english')) # find stop words in English language\n",
    "    lemmatizer = WordNetLemmatizer() # declare nltk lemmatizer\n",
    "\n",
    "    # iterate through every sentence and replace it by itself lemmatized, without punctuation and without stop words\n",
    "    for i in range(len(data)):\n",
    "        sentence_no_punct = ''\n",
    "        # remove punctuation\n",
    "        \n",
    "        for char in data[i]:\n",
    "            if char not in string.punctuation:\n",
    "                sentence_no_punct = sentence_no_punct + char\n",
    "        data[i] = sentence_no_punct\n",
    "\n",
    "        word_tokens = word_tokenize(data[i])\n",
    "    \n",
    "        # remove stop words and lemmatize\n",
    "        word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words and len(word) > 1]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'v') for word in word_tokens]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'a') for word in word_tokens]\n",
    "        \n",
    "        # remove conjunction words\n",
    "        word_tokens = [word for word in word_tokens if word[-2:] != 'nt']\n",
    "        (data[i]) = ' '.join(word_tokens)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def StandardScaler (data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    return scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENTS\n",
    "\n",
    "train_descs = parse_descriptions('cs5785-fall19-final/descriptions_train', 10000)\n",
    "test_descs  = parse_descriptions('cs5785-fall19-final/descriptions_test', 2000)\n",
    "\n",
    "train_descs = preprocessing(train_descs)\n",
    "test_descs  = preprocessing(test_descs)\n",
    "\n",
    "# TAGS\n",
    "\n",
    "train_tags = parse_tags(parse_descriptions(\"cs5785-fall19-final/tags_train\", num_doc=10000))\n",
    "test_tags = parse_tags(parse_descriptions(\"cs5785-fall19-final/tags_test\", num_doc=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.read_csv(\"cs5785-fall19-final/features_train/features_resnet1000_train.csv\", header = None, index_col=None)\n",
    "test_feat = pd.read_csv(\"cs5785-fall19-final/features_test/features_resnet1000_test.csv\", header = None, index_col=None)\n",
    "\n",
    "train_feat_intern = pd.read_csv(\"cs5785-fall19-final/features_train/features_resnet1000intermediate_train.csv\", header = None, index_col=None)\n",
    "test_feat_intern = pd.read_csv(\"cs5785-fall19-final/features_test/features_resnet1000intermediate_test.csv\", header = None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_feat[0])):\n",
    "    train_feat[0][i] = int(train_feat[0][i].replace(\"images_train/\", \"\").replace(\".jpg\", \"\"))    \n",
    "train_feat_sort = train_feat.sort_values(by=0)\n",
    "\n",
    "for i in range(len(test_feat[0])):\n",
    "    test_feat[0][i] = int(test_feat[0][i].replace(\"images_test/\", \"\").replace(\".jpg\", \"\"))\n",
    "test_feat_sort = test_feat.sort_values(by=0)\n",
    "\n",
    "\n",
    "for i in range(len(train_feat_intern[0])):\n",
    "    train_feat_intern[0][i] = int(train_feat_intern[0][i].replace(\"images_train/\", \"\").replace(\".jpg\", \"\"))    \n",
    "train_feat_sort_intern = train_feat_intern.sort_values(by=0)\n",
    "\n",
    "for i in range(len(test_feat_intern[0])):\n",
    "    test_feat_intern[0][i] = int(test_feat_intern[0][i].replace(\"images_test/\", \"\").replace(\".jpg\", \"\"))\n",
    "test_feat_sort_intern = test_feat_intern.sort_values(by=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sort = train_feat_sort.drop(columns=0).to_numpy()\n",
    "test_feat_sort = test_feat_sort.drop(columns=0).to_numpy()\n",
    "\n",
    "\n",
    "train_feat_sort_intern = train_feat_sort_intern.drop(columns=0).to_numpy()\n",
    "test_feat_sort_intern = test_feat_sort_intern.drop(columns=0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(train_feat_sort_intern).to_csv(\"train_feat_sort_intern.csv\")\n",
    "#pd.DataFrame(test_feat_sort_intern).to_csv(\"test_feat_sort_intern.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA NORMALIZATION\n",
    "train_feat_sort = StandardScaler(train_feat_sort)\n",
    "test_feat_sort = StandardScaler(test_feat_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "#def doc_to_vec(sentence, word2vec):\n",
    "#    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "#    return np.stack(word_vecs).mean(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD2VEC CONCATENATING MAX AND MIN\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    #maxim = np.stack(word_vecs).max(0)\n",
    "    #minim = np.stack(word_vecs).max(0)\n",
    "    #return np.concatenate((maxim, minim), axis=0)\n",
    "    return np.stack(word_vecs).max(0) + np.stack(word_vecs).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTIONS\n",
    "x_train = np.array([doc_to_vec(train_descs[i], word2vec) for i in range(len(train_descs))])\n",
    "x_test = np.array([doc_to_vec(d, word2vec) for d in test_descs])\n",
    "\n",
    "# DATA NORMALIZATION\n",
    "x_train = StandardScaler(x_train)\n",
    "x_test = StandardScaler(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAGS\n",
    "x_train_tags = np.array([doc_to_vec(train_descs[i], word2vec) for i in range(len(train_tags))])\n",
    "x_test_tags = np.array([doc_to_vec(d, word2vec) for d in test_tags])\n",
    "\n",
    "# DATA NORMALIZATION\n",
    "x_train_tags = StandardScaler(x_train_tags)\n",
    "x_test_tags = StandardScaler(x_test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=70, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 70)\n",
    "pca.fit(train_feat_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pca.transform(train_feat_sort)\n",
    "y_test = pca.transform(test_feat_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model(s) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='adam', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1, max_iter = 1000)\n",
    "clf.fit(xtrain, ytrain)\n",
    "yval_pred = clf.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRESULTS: \\nDevelopment MAP@20: 0.15715323095892755 [adam, hidden_layer_sizes = (100,) // PCA = 100 components]\\n\\n'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RESULTS: \n",
    "Development MAP@20: 0.15715323095892755 [adam, hidden_layer_sizes = (100,) // PCA = 100 components]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def kernel_ridge(xtrain, ytrain):\n",
    "    kr = KernelRidge()\n",
    "#     kr.fit(train_bow, train_feat_pca)\n",
    "    kr.fit(xtrain, ytrain)\n",
    "    return kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE NEEDED TO RUN ALL THE CODE\n",
    "# kr = kernel_ridge(train_bow, train_feat_pca)\n",
    "# preds = kr.predict(test_bow)\n",
    "# dm = dist_matrix(preds, test_feat_pca)\n",
    "# top_images = output_format(dm)\n",
    "# outputCSV(top_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr2 = kernel_ridge(xtrain, ytrain)\n",
    "yval_pred = kr2.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDevelopment MAP@20: 0.18237442016359973 [PCA = 50 components]\\nDevelopment MAP@20: 0.1928452635084214  [PCA = 70 components] ** BEST\\nDevelopment MAP@20: 0.19259939057502526 [PCA = 100 components]\\nDevelopment MAP@20: 0.19369393946438837 [PCA = 100 components]\\nDevelopment MAP@20: 0.18889584037737286 [PCA = 300 components]\\n\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Development MAP@20: 0.18237442016359973 [PCA = 50 components]\n",
    "Development MAP@20: 0.1928452635084214  [PCA = 70 components] ** BEST\n",
    "Development MAP@20: 0.19259939057502526 [PCA = 100 components]\n",
    "Development MAP@20: 0.19369393946438837 [PCA = 100 components]\n",
    "Development MAP@20: 0.18889584037737286 [PCA = 300 components]\n",
    "\n",
    "Concatenating Max and Min in word2vec:\n",
    "Development MAP@20: 0.142785998571432 [PCA = 70 components]\n",
    "\n",
    "Adding Max and Min in word2vec:\n",
    "Development MAP@20: 0.11136266005752075 [PCA = 70 components]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge with Tags - WRONG APPROACH! Tags belong to images. Needs to be changed. However, why did I get this accuracy?!?!? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(x_train_tags, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def kernel_ridge(xtrain, ytrain):\n",
    "    kr = KernelRidge()\n",
    "#     kr.fit(train_bow, train_feat_pca)\n",
    "    kr.fit(xtrain, ytrain)\n",
    "    return kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr2 = kernel_ridge(xtrain, ytrain)\n",
    "yval_pred = kr2.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDevelopment MAP@20: 0.18203206580485218 [PCA = ]\\n'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Development MAP@20: 0.18799833563753068 [PCA = 70 components]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_matrix(x1, x2):\n",
    "    return cdist(x1,x2,'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_distances = dist_matrix(yval_pred, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.11136266005752075\n",
      "Mean index of true image 104.3556\n",
      "Median index of true image 33.0\n"
     ]
    }
   ],
   "source": [
    "val_scores = []\n",
    "val_pos_list = []\n",
    "\n",
    "for i in range(len(yval)):\n",
    "    pred_dist_idx = list(np.argsort(val_distances[i]))\n",
    "    val_pos = pred_dist_idx.index(i)\n",
    "    val_pos_list.append(val_pos)\n",
    "    if val_pos < 20:\n",
    "        val_scores.append(1 / (val_pos + 1))\n",
    "    else:\n",
    "        val_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(val_scores))\n",
    "print(\"Mean index of true image\", np.mean(val_pos_list))\n",
    "print(\"Median index of true image\", np.median(val_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
