{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we are trying to train a lot of different models using word2vec as an encriptor for the X (the descriptions).\n",
    "\n",
    "The document is structred as follows:\n",
    "\n",
    "- **Load data**\n",
    "    - Descriptions (x)\n",
    "    - Features (y)\n",
    "    \n",
    "- **Word2Vec (for descriptions)**\n",
    "- **PCA (for features)**\n",
    "\n",
    "- **Model(s)** training *make sure to comment the best results of each method with its hyperparameters*\n",
    "    1. MLP\n",
    "    2. XX\n",
    "\n",
    "- **Accuracy in the validation set**\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "Ideas of things to try:\n",
    "\n",
    "- N grams\n",
    "- Keras\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "# function to preprocess data\n",
    "def preprocessing(data):\n",
    "    stop_words = set(stopwords.words('english')) # find stop words in English language\n",
    "    lemmatizer = WordNetLemmatizer() # declare nltk lemmatizer\n",
    "\n",
    "    # iterate through every sentence and replace it by itself lemmatized, without punctuation and without stop words\n",
    "    for i in range(len(data)):\n",
    "        sentence_no_punct = ''\n",
    "        # remove punctuation\n",
    "        \n",
    "        for char in data[i]:\n",
    "            if char not in string.punctuation:\n",
    "                sentence_no_punct = sentence_no_punct + char\n",
    "        data[i] = sentence_no_punct\n",
    "\n",
    "        word_tokens = word_tokenize(data[i])\n",
    "    \n",
    "        # remove stop words and lemmatize\n",
    "        word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words and len(word) > 1]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'v') for word in word_tokens]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'a') for word in word_tokens]\n",
    "        \n",
    "        # remove conjunction words\n",
    "        word_tokens = [word for word in word_tokens if word[-2:] != 'nt']\n",
    "        (data[i]) = ' '.join(word_tokens)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def StandardScaler (data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    return scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descs = parse_descriptions('cs5785-fall19-final/descriptions_train', 10000)\n",
    "test_descs  = parse_descriptions('cs5785-fall19-final/descriptions_test', 2000)\n",
    "\n",
    "train_descs = preprocessing(train_descs)\n",
    "test_descs  = preprocessing(test_descs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.read_csv(\"cs5785-fall19-final/features_train/features_resnet1000_train.csv\", header = None, index_col=None)\n",
    "test_feat = pd.read_csv(\"cs5785-fall19-final/features_test/features_resnet1000_test.csv\", header = None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_feat[0])):\n",
    "    train_feat[0][i] = int(train_feat[0][i].replace(\"images_train/\", \"\").replace(\".jpg\", \"\"))    \n",
    "train_feat_sort = train_feat.sort_values(by=0)\n",
    "\n",
    "for i in range(len(test_feat[0])):\n",
    "    test_feat[0][i] = int(test_feat[0][i].replace(\"images_test/\", \"\").replace(\".jpg\", \"\"))\n",
    "test_feat_sort = test_feat.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sort = train_feat_sort.drop(columns=0).to_numpy()\n",
    "test_feat_sort = test_feat_sort.drop(columns=0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA NORMALIZATION\n",
    "train_feat_sort = StandardScaler(train_feat_sort)\n",
    "test_feat_sort = StandardScaler(test_feat_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec - Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vec(sentence, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    return np.stack(word_vecs).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([doc_to_vec(train_descs[i], word2vec) for i in range(len(train_descs))])\n",
    "x_test = np.array([doc_to_vec(d, word2vec) for d in test_descs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA NORMALIZATION\n",
    "x_train = StandardScaler(x_train)\n",
    "x_test = StandardScaler(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=100, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 100)\n",
    "pca.fit(train_feat_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pca.transform(train_feat_sort)\n",
    "y_test = pca.transform(test_feat_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model(s) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xval, ytrain, yval = train_test_split(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='adam', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1, max_iter = 1000)\n",
    "\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "yval_pred = clf.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RESULTS: \n",
    "Development MAP@20: 0.15715323095892755 [adam, hidden_layer_sizes = (100,)]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def dist_matrix(x1, x2):\n",
    "    return cdist(x1,x2,'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_distances = dist_matrix(yval_pred, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.15715323095892755\n",
      "Mean index of true image 78.9964\n",
      "Median index of true image 22.0\n"
     ]
    }
   ],
   "source": [
    "val_scores = []\n",
    "val_pos_list = []\n",
    "\n",
    "for i in range(len(yval)):\n",
    "    pred_dist_idx = list(np.argsort(val_distances[i]))\n",
    "    val_pos = pred_dist_idx.index(i)\n",
    "    val_pos_list.append(val_pos)\n",
    "    if val_pos < 20:\n",
    "        val_scores.append(1 / (val_pos + 1))\n",
    "    else:\n",
    "        val_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(val_scores))\n",
    "print(\"Mean index of true image\", np.mean(val_pos_list))\n",
    "print(\"Median index of true image\", np.median(val_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
