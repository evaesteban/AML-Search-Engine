{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/cs5785-fall19-final/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "import csv\n",
    "import sklearn\n",
    "stopWords = stopwords.words('english')\n",
    "isStopWord = lambda w: w in stopWords or len(w) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = \"cs5785-fall19-final\"\n",
    "\n",
    "descTrainFolder = dataFolder + \"/descriptions_train\"\n",
    "descTestFolder = dataFolder + \"/descriptions_test\"\n",
    "\n",
    "featTrainFolder = dataFolder + \"/features_train\"\n",
    "featTestFolder = dataFolder + \"/features_test\"\n",
    "\n",
    "imagesTrainFolder = dataFolder + \"/images_train\"\n",
    "imagesTestFolder = dataFolder + \"/images_test\"\n",
    "\n",
    "tagsTrainFolder = dataFolder + \"/tags_train\"\n",
    "tagsTestFolder = dataFolder + \"/tags_test\"\n",
    "\n",
    "folders = [descTrainFolder,   descTestFolder,   featTrainFolder, featTestFolder, \n",
    "           imagesTrainFolder, imagesTestFolder, tagsTrainFolder, tagsTestFolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesFromFolder(folder):\n",
    "    return listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data\n",
    "def preprocessing(data):\n",
    "    stop_words = set(stopwords.words('english')) # find stop words in English language\n",
    "    lemmatizer = WordNetLemmatizer() # declare nltk lemmatizer\n",
    "\n",
    "    # iterate through every sentence and replace it by itself lemmatized, without punctuation and without stop words\n",
    "    for i in range(len(data)):\n",
    "        sentence_no_punct = ''\n",
    "        # remove punctuation\n",
    "        \n",
    "        for char in data[i]:\n",
    "            if char not in string.punctuation:\n",
    "                sentence_no_punct = sentence_no_punct + char\n",
    "        data[i] = sentence_no_punct\n",
    "\n",
    "        word_tokens = word_tokenize(data[i])\n",
    "    \n",
    "        # remove stop words and lemmatize\n",
    "        word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words and len(word) > 1]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'v') for word in word_tokens]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'a') for word in word_tokens]\n",
    "        \n",
    "        # remove conjunction words\n",
    "        word_tokens = [word for word in word_tokens if word[-2:] != 'nt']\n",
    "        (data[i]) = ' '.join(word_tokens)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_descriptions_from_folder(folder):\n",
    "    descriptions = []\n",
    "    for i in range(10000):\n",
    "        with open(('cs5785-fall19-final/descriptions_train/{}.txt').format(i), newline='') as f:\n",
    "            desc1 = []\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                desc1.append(row)\n",
    "            descriptions.append(desc1)\n",
    "    \n",
    "    # FIRST FLATTENING\n",
    "    descriptions2 = []\n",
    "    for description in descriptions:\n",
    "        descriptions2.append([desc for sublist in description for desc in sublist])\n",
    "        \n",
    "    # MADE THE SENTENCES ALL ONE FOR EACH DESCRIPTION FILE\n",
    "    flat_descriptions = []\n",
    "    for description in descriptions2:\n",
    "        desc1 = []\n",
    "        for sentence in description:\n",
    "            desc1 += sentence.split(' ')\n",
    "        flat_descriptions.append(desc1)\n",
    "    \n",
    "    for i in range(len(flat_descriptions)):\n",
    "        flat_descriptions[i] = (' ').join(flat_descriptions[i]).lower()\n",
    "    \n",
    "    return flat_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_descs_train = get_flat_descriptions_from_folder(descTrainFolder)\n",
    "flat_descs_test = get_flat_descriptions_from_folder(descTestFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_descs = [' '.join(set(preprocessing(desc).split())) for desc in flat_descs_train]\n",
    "#test_descs = [' '.join(set(preprocessing(desc).split())) for desc in flat_descs_test]\n",
    "\n",
    "# preprocess\n",
    "train_descs = preprocessing(flat_descs_train)\n",
    "test_descs = preprocessing(flat_descs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "resTrainFile = featTrainFolder + \"/features_resnet1000_train.csv\"\n",
    "train_feat = pd.read_csv(resTrainFile, header = None, index_col = None)\n",
    "\n",
    "# testing data\n",
    "resTrainFile = featTestFolder + \"/features_resnet1000_test.csv\"\n",
    "test_feat = pd.read_csv(resTrainFile, header = None, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train logistic regression model, predict with it, and calculate the accuracy and confusion matrix\n",
    "def Logistic_Regression(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # initialize and fit logistic regression model with training data\n",
    "    lr = LogisticRegression(solver = 'lbfgs', max_iter = 10000)\n",
    "    lr.fit(x_train, y_train)\n",
    "    \n",
    "    # predict the result for the testing data\n",
    "    lr_pred = lr.predict(x_test) \n",
    "    \n",
    "    # calculate accuracy\n",
    "    lr_acc = accuracy_score(lr_pred, y_test)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cfn_matrix_lr = confusion_matrix(y_test, lr_pred)\n",
    "   \n",
    "    # return accuracy and confusion matrix\n",
    "    return lr_acc,cfn_matrix_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train gaussian naive bayes model, predict with it, and calculate the accuracy and confusion matrix\n",
    "def Gaussian_NB(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # initialize and fit naive bayes model gaussian prior with training data\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y_train)\n",
    "\n",
    "    # predict the results for the test set\n",
    "    gnb_pred = gnb.predict(x_test) \n",
    "    \n",
    "    # calculate accuracy\n",
    "    gnb_acc = accuracy_score(gnb_pred, y_test)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cfn_matrix_gnb = confusion_matrix(y_test, gnb_pred)\n",
    "    \n",
    "    # return accuracy and confusion matrix\n",
    "    return gnb_acc, cfn_matrix_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forest regressor model, predict with it, and calculate the accuracy and confusion matrix\n",
    "def Random_Forest(x_train, y_train, x_test, y_test):\n",
    "    # set up regressor  \n",
    "    rf_regressor = RandomForestRegressor(max_depth=20) \n",
    "    \n",
    "    # fit regressor \n",
    "    rf_regressor.fit(x_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    rf_pred = rf_regressor.predict(x_test)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    rf_acc = accuracy_score(rf_pred, y_test)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cfn_matrix_rf = confusion_matrix(y_test, rf_pred)\n",
    "    \n",
    "    # return accuracy and confusion matrix\n",
    "    return rf_acc, cfn_matrix_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train KNN regressor model, predict with it, and calculate the accuracy and confusion matrix\n",
    "def KNN_Regressor(x_train, y_train, x_test, y_test):\n",
    "    # set up regressor  \n",
    "    knn_regressor = KNeighborsRegressor(max_depth=20) \n",
    "    \n",
    "    # fit regressor \n",
    "    knn_regressor.fit(x_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    knn_pred = knn_regressor.predict(x_test)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    knn_acc = accuracy_score(knn_pred, y_test)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cfn_matrix_knn = confusion_matrix(y_test, knn_pred)\n",
    "    \n",
    "    # return accuracy and confusion matrix\n",
    "    return knn_acc, cfn_matrix_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the number of nearest neighbors specified\n",
    "def knn_function(data, neighbors_number):\n",
    "    nbrs = NearestNeighbors(n_neighbors=neighbors_number, algorithm='ball_tree').fit(data)\n",
    "    return nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE BAG OF WORDS DICTIONARY\n",
    "BOW = {}\n",
    "for description in train_descs:\n",
    "    sentence_lst = description.split(' ')\n",
    "    for word in sentence_lst:\n",
    "        BOW[word] = 0\n",
    "BOW['null'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FEATURE VECTORS for TRAIN\n",
    "feature_vectors = []\n",
    "for description in train_descs:\n",
    "    feat_vec = BOW.copy()\n",
    "    sentence_lst = description.split(' ')\n",
    "    for word in sentence_lst:\n",
    "        if word in feat_vec:\n",
    "            feat_vec[word] += 1\n",
    "        else:\n",
    "            feat_vec['null'] += 1       \n",
    "    feature_vectors.append(feat_vec)\n",
    "    \n",
    "# TURN DICTIONARIES INTO A MATRIX with each row as one description\n",
    "feature_vector_matrix = []\n",
    "for feature_vec in feature_vectors:\n",
    "    feature_vector_matrix.append(list(feature_vec.values()))\n",
    "    \n",
    "# NORMALIZE THE FEATURES\n",
    "feature_vector_matrix = sklearn.preprocessing.normalize(feature_vector_matrix) # default is L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FEATURE VECTORS for TEST\n",
    "feature_vectors = []\n",
    "for description in test_descs:\n",
    "    feat_vec = BOW.copy()\n",
    "    sentence_lst = description.split(' ')\n",
    "    for word in sentence_lst:\n",
    "        if word in feat_vec:\n",
    "            feat_vec[word] += 1\n",
    "        else:\n",
    "            feat_vec['null'] += 1       \n",
    "    feature_vectors.append(feat_vec)\n",
    "    \n",
    "# TURN DICTIONARIES INTO A MATRIX with each row as one description\n",
    "feature_vector_matrix_test = []\n",
    "for feature_vec in feature_vectors:\n",
    "    feature_vector_matrix_test.append(list(feature_vec.values()))\n",
    "    \n",
    "# NORMALIZE THE FEATURES\n",
    "feature_vector_matrix_test = sklearn.preprocessing.normalize(feature_vector_matrix_test) # default is L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6798)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1001)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skateboard use man person picnic top skate table crowd pull watch stage show rid boarder put trick skateboarder'\n",
      " 'enjoy next someone soup noodle bowl ready serve healthy food asian eat sit chopstick tasty carrot ramen shrimp'\n",
      " 'man intersection cross drive bus ice icecream concession near truck cream street walk across behind busy'\n",
      " ...\n",
      " 'lamp background bedroom cover small nightstand behind room stand next table sit bunch night wall quilt type paper bed'\n",
      " 'track station drive metal next wall rail train past silver passenger hour travel sit daytime across subway'\n",
      " 'man board wave surf huge top big surfboard hit shirt rid white trick wipe fall surfer water']\n",
      "[7770 1631 4256 ... 3195 9115 4176]\n"
     ]
    }
   ],
   "source": [
    "# method to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#le.fit(train_descs)\n",
    "#encoding = le.transform(train_descs)\n",
    "#print(le.inverse_transform(encoding))\n",
    "#print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (10000, 1001)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-adb3d8dc3329>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfn_matrix_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogistic_Regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_vector_matrix_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#gnb_acc, cfn_matrix_gnb = Gaussian_NB(train_descs, train_feat, test_descs, test_feat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#rf_acc, cfn_matrix_rf = Random_Forest(train_descs, train_feat, test_descs, test_feat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-1dc5d80cb04e>\u001b[0m in \u001b[0;36mLogistic_Regression\u001b[1;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# initialize and fit logistic regression model with training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# predict the result for the testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    722\u001b[0m                         dtype=None)\n\u001b[0;32m    723\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (10000, 1001)"
     ]
    }
   ],
   "source": [
    "lr_acc, cfn_matrix_lr = Logistic_Regression(feature_vector_matrix, train_feat, feature_vector_matrix_test, test_feat)\n",
    "#gnb_acc, cfn_matrix_gnb = Gaussian_NB(train_descs, train_feat, test_descs, test_feat)\n",
    "#rf_acc, cfn_matrix_rf = Random_Forest(train_descs, train_feat, test_descs, test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_feat.drop(train_feat.columns[0], axis=1)\n",
    "#test = ((((train_feat.loc[:,0]).split('/'))[1]).split('.'))[0]\n",
    "#test = train_feat.loc[:,0]\n",
    "#test_2 = (((test.split('/'))[1]).split('.'))[0]\n",
    "#train_feat[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
